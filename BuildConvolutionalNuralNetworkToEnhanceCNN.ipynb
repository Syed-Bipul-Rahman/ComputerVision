{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images / 255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#Add another convolution\n",
    "tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version\n",
    "tf.keras.layers.Flatten(),\n",
    "#The same 128 dense layers, and 10 output layers as in the pre-convolution example:\n",
    "tf.keras.layers.Dense(128, activation='relu'),\n",
    "tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_labels[:100])\n",
    "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_acc*100))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
